{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgeY1ir0eyneLVDBPxRsNt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/casllmproject/bending_effect/blob/main/A2_1_SBERT_Topic_Modeling_Simul_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This solution uses the BERTopic library, which is a powerful framework that leverages SBERT embeddings, UMAP for dimensionality reduction, and HDBSCAN for clustering to discover topics. This approach directly integrates SBERT for clustering (step 7) to achieve topic modeling (step 5)."
      ],
      "metadata": {
        "id": "NPhkSSH85crW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYfs0zxqoyqm"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block 1: Install Libraries and Mount Drive\n",
        "This block installs the necessary packages (bertopic, sentence-transformers, and their dependencies) and mounts Google Drive to access the dataset."
      ],
      "metadata": {
        "id": "7lyDiW3T5a_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import silhouette_score\n",
        "from hdbscan import HDBSCAN\n",
        "from bertopic import BERTopic\n",
        "from google.colab import drive\n",
        "import plotly.offline as pyo\n",
        "\n",
        "# Enable Plotly for Colab\n",
        "pyo.init_notebook_mode(connected=True)"
      ],
      "metadata": {
        "id": "lUUGxRe3pAJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block 2: Load and Prepare Data\n",
        "Here, we load your CSV file, handle any missing text data, and confirm the data structure."
      ],
      "metadata": {
        "id": "7B1b4jca5ij8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to dataset\n",
        "file_path = \"/content/drive/MyDrive/CYON_Analysis_Materials/simulated_responses_re.csv\"\n",
        "\n",
        "# Load the dataset\n",
        "try:\n",
        "    df = pd.read_csv(file_path)\n",
        "\n",
        "    # --- Data Preparation ---\n",
        "    print(f\"Original dataset shape: {df.shape}\")\n",
        "\n",
        "    # Drop rows where the text data is missing\n",
        "    df = df.dropna(subset=['ed_generatedBody'])\n",
        "\n",
        "    # Ensure the text column is treated as a string\n",
        "    df['ed_generatedBody'] = df['ed_generatedBody'].astype(str)\n",
        "\n",
        "    print(f\"Cleaned dataset shape: {df.shape}\")\n",
        "\n",
        "    # Display the first few rows and column info\n",
        "    print(\"\\nDataFrame Head:\")\n",
        "    display(df.head())\n",
        "\n",
        "    print(\"\\nDataFrame Info:\")\n",
        "    df.info()\n",
        "\n",
        "    print(\"\\nUnique values in grouping columns:\")\n",
        "    print(f\"gr_per: {df['gr_per'].nunique()} categories\")\n",
        "    print(f\"Group: {df['Group'].nunique()} categories\")\n",
        "    print(f\"DEM8: {df['DEM8'].nunique()} categories\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: File not found at {file_path}\")\n",
        "    print(\"Please check the file path and Google Drive permissions.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the data: {e}\")"
      ],
      "metadata": {
        "id": "ZBfVWngkpBzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block 3: Initialize SBERT Model\n",
        "Load the SBERT model once here. I'll use 'all-MiniLM-L6-v2', which is a high-performance, fast model suitable for this task."
      ],
      "metadata": {
        "id": "A-sabCmq5kL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pre-trained SBERT model\n",
        "# This model will be used to convert all text into numerical embeddings\n",
        "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "print(\"SBERT model 'all-MiniLM-L6-v2' loaded successfully.\")"
      ],
      "metadata": {
        "id": "FP3IjO8vplGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block 4: Define Topic Modeling & Clustering Function\n",
        "To avoid repeating code, I'll create a function that performs the complete analysis for any given grouping variable. This function will:\n",
        "\n",
        "Loop through each unique value in the group column (e.g., 'Democrat', 'Republican').\n",
        "\n",
        "Generate SBERT embeddings for that group's texts.\n",
        "\n",
        "Use BERTopic to cluster the embeddings and identify topics.\n",
        "\n",
        "Store all the results (model, topics, embeddings, etc.) in a dictionary."
      ],
      "metadata": {
        "id": "4mVltIPj5qRV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_topic_analysis(dataframe, text_column, group_column, embedding_model):\n",
        "    \"\"\"\n",
        "    Performs BERTopic analysis for each subgroup within a grouping column.\n",
        "\n",
        "    This updated version:\n",
        "    1. Removes English stop words (\"and\", \"in\", \"of\", etc.).\n",
        "    2. Uses a custom HDBSCAN model with 'leaf' selection\n",
        "       to find many granular, detailed topics.\n",
        "    3. Calculates and reports the Silhouette Score for clustering quality.\n",
        "\n",
        "    Args:\n",
        "        dataframe (pd.DataFrame): The main DataFrame.\n",
        "        text_column (str): The name of the column with text data.\n",
        "        group_column (str): The name of the grouping column (e.g., 'DEM8').\n",
        "        embedding_model (SentenceTransformer): The pre-loaded SBERT model.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary where keys are group names and values are\n",
        "              dictionaries containing the analysis results.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\n--- Starting Analysis for Grouping Variable: {group_column} ---\")\n",
        "\n",
        "    # 1. Define a CountVectorizer to remove English stop words\n",
        "    vectorizer = CountVectorizer(stop_words='english')\n",
        "\n",
        "    # 2. --- NEW: Define a granular HDBSCAN model ---\n",
        "    # set 'cluster_selection_method' to 'leaf' to get the\n",
        "    # most detailed, granular clusters.\n",
        "    # set 'min_cluster_size' to 5 (our new min_topic_size).\n",
        "    hdbscan_model = HDBSCAN(\n",
        "        min_cluster_size=5,\n",
        "        min_samples=2,\n",
        "        cluster_selection_method='leaf',\n",
        "        prediction_data=True\n",
        "    )\n",
        "    # -----------------------------------------------\n",
        "\n",
        "    results_store = {}\n",
        "    unique_groups = dataframe[group_column].unique()\n",
        "\n",
        "    for group in unique_groups:\n",
        "        print(f\"\\nProcessing group: '{group}' (from {group_column})\")\n",
        "\n",
        "        # 3. Filter DataFrame for the current group\n",
        "        group_df = dataframe[dataframe[group_column] == group].copy()\n",
        "        texts = group_df[text_column].tolist()\n",
        "\n",
        "        # 4. Check if there is enough data\n",
        "        min_docs_required = 15 # Still need a reasonable number of docs\n",
        "        if len(texts) < min_docs_required:\n",
        "            print(f\"Skipping group '{group}': only {len(texts)} documents.\")\n",
        "            print(f\"Need at least {min_docs_required} for robust clustering.\")\n",
        "            continue\n",
        "\n",
        "        # 5. Generate SBERT Embeddings\n",
        "        print(f\"Generating embeddings for {len(texts)} documents...\")\n",
        "        embeddings = embedding_model.encode(texts, show_progress_bar=True)\n",
        "\n",
        "        # 6. Initialize and Run BERTopic\n",
        "        topic_model = BERTopic(\n",
        "            verbose=False,\n",
        "            calculate_probabilities=True,\n",
        "            vectorizer_model=vectorizer, # Remove stop words\n",
        "            hdbscan_model=hdbscan_model\n",
        "        )\n",
        "\n",
        "        # 7. Fit the model (this performs clustering)\n",
        "        print(\"Fitting BERTopic model (clustering documents)...\")\n",
        "        topics, probs = topic_model.fit_transform(texts, embeddings=embeddings)\n",
        "\n",
        "        # 8. Calculate Clustering Score (Silhouette Score)\n",
        "        silhouette_avg = None\n",
        "        try:\n",
        "            # Filter out outlier embeddings and labels (topic == -1)\n",
        "            non_outlier_mask = (topics != -1)\n",
        "            if np.sum(non_outlier_mask) > 0:\n",
        "                filtered_embeddings = embeddings[non_outlier_mask]\n",
        "                filtered_labels = np.array(topics)[non_outlier_mask]\n",
        "\n",
        "                # Check if we have more than 1 cluster\n",
        "                if len(np.unique(filtered_labels)) > 1:\n",
        "                    silhouette_avg = silhouette_score(filtered_embeddings, filtered_labels)\n",
        "                    print(f\"Silhouette Score (ignoring outliers): {silhouette_avg:.4f}\")\n",
        "                else:\n",
        "                    print(\"Only one cluster found (or only outliers). Cannot calculate Silhouette Score.\")\n",
        "            else:\n",
        "                print(\"No clusters found (only outliers). Cannot calculate Silhouette Score.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Could not calculate Silhouette Score: {e}\")\n",
        "\n",
        "        # 9. Store all results for this group\n",
        "        group_df['topic'] = topics\n",
        "        results_store[group] = {\n",
        "            'model': topic_model,\n",
        "            'topics': topics,\n",
        "            'probabilities': probs,\n",
        "            'embeddings': embeddings,\n",
        "            'dataframe_with_topics': group_df,\n",
        "            'texts': texts,\n",
        "            'silhouette_score': silhouette_avg\n",
        "        }\n",
        "\n",
        "        num_topics = len(topic_model.get_topic_info()) - 1\n",
        "        print(f\"Finished group '{group}'. Found {num_topics} topics.\")\n",
        "\n",
        "    print(f\"\\n--- Completed Analysis for {group_column} ---\")\n",
        "    return results_store"
      ],
      "metadata": {
        "id": "BWNadyBOprIX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Block 5: Run Analysis for All 3 Grouping Variables."
      ],
      "metadata": {
        "id": "-Haq8ldz5xW2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The results will be stored in separate dictionaries.\n",
        "\n",
        "print(\"Starting analysis for 'gr_per'...\")\n",
        "gr_per_results = perform_topic_analysis(df, 'ed_generatedBody', 'gr_per', sbert_model)\n",
        "\n",
        "print(\"\\nStarting analysis for 'Group'...\")\n",
        "group_results = perform_topic_analysis(df, 'ed_generatedBody', 'Group', sbert_model)\n",
        "\n",
        "print(\"\\nStarting analysis for 'DEM8'...\")\n",
        "dem8_results = perform_topic_analysis(df, 'ed_generatedBody', 'DEM8', sbert_model)\n",
        "\n",
        "print(\"\\n\\n--- ALL ANALYSES COMPLETE ---\")"
      ],
      "metadata": {
        "id": "VIVE458ppuP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Let's inspect the results for the 'Democrat' group ---\n",
        "target_group_name = 'Democrat'\n",
        "target_results = dem8_results # Use dem8_results, group_results, or gr_per_results\n",
        "\n",
        "if target_group_name in target_results:\n",
        "    print(f\"--- Interpreting Topic Modeling Results for: {target_group_name} ---\")\n",
        "\n",
        "    # Get the fitted model and data for this group\n",
        "    model = target_results[target_group_name]['model']\n",
        "\n",
        "    # 1. Get the main Topic Info DataFrame\n",
        "    # Topic -1 consists of outliers (texts that didn't fit any cluster).\n",
        "    topic_info_df = model.get_topic_info()\n",
        "    print(f\"Found {len(topic_info_df)-1} topics for '{target_group_name}'.\")\n",
        "    print(\"Top 10 Topics (by size):\")\n",
        "    display(topic_info_df.head(11))\n",
        "\n",
        "    # 2. Get the words for a specific topic (e.g., Topic 0)\n",
        "    print(\"\\nWords and scores for Topic 0 (the largest topic):\")\n",
        "    print(model.get_topic(0))\n",
        "\n",
        "    # 3. Visualize Topic Word Scores (Interactive Bar Chart)\n",
        "    # This shows the most important words for each topic.\n",
        "    print(\"\\nGenerating interactive Topic Word Bar Chart...\")\n",
        "    # This plot is interactive: you can hover to see scores.\n",
        "    fig_bar = model.visualize_barchart(top_n_topics=10) # Show top 10 topics\n",
        "    fig_bar.show()\n",
        "\n",
        "    # 4. Visualize Topic Hierarchy (Interactive Dendrogram)\n",
        "    # This shows how topics (clusters) relate to each other.\n",
        "    print(\"\\nGenerating interactive Topic Hierarchy...\")\n",
        "    fig_hierarchy = model.visualize_hierarchy(top_n_topics=20) # Show 20 topics\n",
        "    fig_hierarchy.show()\n",
        "\n",
        "else:\n",
        "    print(f\"Group '{target_group_name}' was not processed.\")\n",
        "    print(\"This might be due to insufficient data (less than 15 documents).\")\n",
        "    print(f\"Available processed groups: {list(target_results.keys())}\")"
      ],
      "metadata": {
        "id": "VdEDWNYfp0q4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Let's inspect the results for the 'Republican' group ---\n",
        "target_group_name = 'Republican'\n",
        "target_results = dem8_results # Use dem8_results, group_results, or gr_per_results\n",
        "\n",
        "if target_group_name in target_results:\n",
        "    print(f\"--- Interpreting Topic Modeling Results for: {target_group_name} ---\")\n",
        "\n",
        "    # Get the fitted model and data for this group\n",
        "    model = target_results[target_group_name]['model']\n",
        "\n",
        "    # 1. Get the main Topic Info DataFrame\n",
        "    # This is the easiest way to see all topics.\n",
        "    # Topic -1 consists of outliers (texts that didn't fit any cluster).\n",
        "    topic_info_df = model.get_topic_info()\n",
        "    print(f\"Found {len(topic_info_df)-1} topics for '{target_group_name}'.\")\n",
        "    print(\"Top 10 Topics (by size):\")\n",
        "    display(topic_info_df.head(11))\n",
        "\n",
        "    # 2. Get the words for a specific topic (e.g., Topic 0)\n",
        "    print(\"\\nWords and scores for Topic 0 (the largest topic):\")\n",
        "    print(model.get_topic(0))\n",
        "\n",
        "    # 3. Visualize Topic Word Scores (Interactive Bar Chart)\n",
        "    # This shows the most important words for each topic.\n",
        "    print(\"\\nGenerating interactive Topic Word Bar Chart...\")\n",
        "    # This plot is interactive: you can hover to see scores.\n",
        "    fig_bar = model.visualize_barchart(top_n_topics=10) # Show top 10 topics\n",
        "    fig_bar.show()\n",
        "\n",
        "    # 4. Visualize Topic Hierarchy (Interactive Dendrogram)\n",
        "    # This shows how topics (clusters) relate to each other.\n",
        "    print(\"\\nGenerating interactive Topic Hierarchy...\")\n",
        "    fig_hierarchy = model.visualize_hierarchy(top_n_topics=20) # Show 20 topics\n",
        "    fig_hierarchy.show()\n",
        "\n",
        "else:\n",
        "    print(f\"Group '{target_group_name}' was not processed.\")\n",
        "    print(\"This might be due to insufficient data (less than 15 documents).\")\n",
        "    print(f\"Available processed groups: {list(target_results.keys())}\")"
      ],
      "metadata": {
        "id": "9yXUXxx3uxC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "_df_7['Count'].plot(kind='line', figsize=(8, 4), title='Count')\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "9wnPVjpZvblI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "figsize = (12, 1.2 * len(_df_9['Name'].unique()))\n",
        "plt.figure(figsize=figsize)\n",
        "sns.violinplot(_df_9, x='Count', y='Name', inner='stick', palette='Dark2')\n",
        "sns.despine(top=True, right=True, bottom=True, left=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "sdcdJgFJvVwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "figsize = (12, 1.2 * len(_df_8['Name'].unique()))\n",
        "plt.figure(figsize=figsize)\n",
        "sns.violinplot(_df_8, x='Topic', y='Name', inner='stick', palette='Dark2')\n",
        "sns.despine(top=True, right=True, bottom=True, left=True)"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Pq_Nw0PyvVDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Define the target results dictionary ---\n",
        "target_results = group_results # Use the results from the 'Group' analysis\n",
        "target_categories = [0, 1, 2, 3] # The categories you want to loop through\n",
        "\n",
        "print(f\"--- Starting Batch Analysis for 'Group' variable ---\")\n",
        "\n",
        "for group_category in target_categories:\n",
        "\n",
        "    print(f\"\\n========================================================\")\n",
        "    print(f\"--- Starting Analysis for Group Category: {group_category} ---\")\n",
        "    print(f\"========================================================\")\n",
        "\n",
        "    if group_category in target_results:\n",
        "\n",
        "        # --- Block 6: Interpret Topic Modeling Results ---\n",
        "        print(f\"\\n--- Interpreting Topics for: {group_category} ---\")\n",
        "\n",
        "        model = target_results[group_category]['model']\n",
        "\n",
        "        # 1. Get the main Topic Info DataFrame\n",
        "        topic_info_df = model.get_topic_info()\n",
        "        print(f\"Found {len(topic_info_df)-1} topics for '{group_category}'.\")\n",
        "        print(\"Top 10 Topics (by size):\")\n",
        "        display(topic_info_df.head(11))\n",
        "\n",
        "        # 2. Visualize Topic Word Scores (Interactive Bar Chart)\n",
        "        print(\"\\nGenerating interactive Topic Word Bar Chart...\")\n",
        "        fig_bar = model.visualize_barchart(top_n_topics=10, title=f\"Top Topics for Group {group_category}\")\n",
        "        fig_bar.show()\n",
        "\n",
        "        # 3. Visualize Topic Hierarchy (Interactive Dendrogram)\n",
        "        print(\"\\nGenerating interactive Topic Hierarchy...\")\n",
        "        fig_hierarchy = model.visualize_hierarchy(top_n_topics=20, title=f\"Topic Hierarchy for Group {group_category}\")\n",
        "        fig_hierarchy.show()\n",
        "\n",
        "        # --- Block 7: Visualizing Text Clustering Results ---\n",
        "        print(f\"\\n--- Visualizing Text Clusters for: {group_category} ---\")\n",
        "\n",
        "        # Get the necessary data we stored\n",
        "        texts_to_plot = target_results[group_category]['texts']\n",
        "        embeddings_to_plot = target_results[group_category]['embeddings']\n",
        "\n",
        "        # 4. Visualize Document Clusters (Interactive 2D Scatter Plot)\n",
        "        print(\"Generating interactive 2D cluster visualization...\")\n",
        "        fig_clusters = model.visualize_documents(\n",
        "            texts_to_plot,\n",
        "            embeddings=embeddings_to_plot,\n",
        "            width=900,\n",
        "            height=700,\n",
        "            title=f\"Text Clusters for Group {group_category}\"\n",
        "        )\n",
        "        fig_clusters.show()\n",
        "\n",
        "    else:\n",
        "        print(f\"\\nGroup '{group_category}' was not processed.\")\n",
        "        print(\"This might be due to insufficient data.\")\n",
        "\n",
        "    print(f\"--- Finished Analysis for Group Category: {group_category} ---\")\n",
        "\n",
        "print(\"\\n--- All 'Group' variable analyses complete ---\")"
      ],
      "metadata": {
        "id": "YE4VNT8SvMj9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
